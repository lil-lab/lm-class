import os
import random

import torch
from torch.utils.data import DataLoader, Dataset

import re
from tokenizers import Tokenizer

from typing import List, Any, Tuple

def load_data_ngram(tokenization_level: str):
    """
    Function for loading data for language modeling and WER computation for the n-gram models. You
    may modify the function header and outputs as necessary.

    Inputs:
        tokenization_level (str): The level at which to tokenize the input
    """
    # TODO
    return [], [], [], [], [], []



        
