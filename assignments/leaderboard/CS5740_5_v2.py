import argparse
import base64
import concurrent.futures
import dataclasses
import logging
import os
import pickle
import pprint
from io import StringIO
from typing import Any, Dict, List

import numpy as np
import pandas as pd
import pytz
from github import Github, Organization, Repository
from tqdm import tqdm

ept = pytz.timezone("US/Eastern")

LEADERBOARD_REPO_NAME = "leaderboards"
CLASS = "cornell-cs5740-sp25"

STAFF = {
    "yoavartzi",
    "GSYfate",
    "chenzizhao",
    "evan-wang-13",
    "shankarp8",
    "yilun-hua",
}


## A5 specific ones


def load_sql(sql_path: str) -> List[str]:
    with open(sql_path, "r") as f:
        queries = f.readlines()
    return queries


def load_records(record_path: str):
    with open(record_path, "rb") as f:
        records, error_msgs = pickle.load(f)
    return records


def compute_record_F1(gt_records: List[Any], model_records: List[Any]):
    """
    Note: this is the exact same code from 'utils.py' file in the starter code.
    Copied here to avoid potential import issues.

    Helper function to compute F1 between records
    generated by ground-truth and model SQL queries
    """
    F1s = []
    for gt_rec, model_rec in zip(gt_records, model_records):
        gt_set = set(gt_rec)
        model_set = set(model_rec)

        precision_total = len(model_set)
        if precision_total == 0:
            precision = 1
        else:
            precision = (
                len([rec for rec in model_set if rec in gt_set])
                / precision_total
            )

        recall_total = len(gt_set)
        if recall_total == 0:
            recall = 1
        else:
            recall = (
                len([rec for rec in gt_set if rec in model_set]) / recall_total
            )

        F1 = 2 * precision * recall / (precision + recall + 1e-8)
        F1s.append(F1)

    return np.mean(F1s)


## general utils


@dataclasses.dataclass
class Submission:
    name: str
    members: List[str]
    repo: Repository
    result_files: Dict[str, Any] | None = None
    result_files_loaded: Dict[str, pd.DataFrame] | None = None

    def __str__(self):
        return pprint.pformat(self)

    def pull_results(self, accepted_file_paths: List[str] = None):
        """step 1. pull results/ directory (IO bound)"""
        accepted_file_paths = set(accepted_file_paths or [])
        self.result_files = {}
        for file_path in accepted_file_paths:
            parent_dir = file_path.split("/", maxsplit=1)[0]
            file_name = file_path.split("/", maxsplit=1)[-1]
            try:
                parent_dir = self.repo.get_contents(parent_dir)
                out = [f for f in parent_dir if f.name == file_name]
                if len(out) != 1:
                    raise KeyError(
                        f"Found {len(out)} files with name {file_name} in {parent_dir} from {self.repo}"
                    )
                self.result_files[file_name] = out[0]
            except KeyError as e:
                print(e)
                pass
        return self

    def pull_result_contents(self):
        """step 2. pull dataframes in results/ directory (IO bound)"""
        if not self.result_files:
            return self

        def _load(path):
            content_encoded = self.repo.get_git_blob(path.sha).content
            content = base64.b64decode(content_encoded)
            comment = path.last_modified_datetime.astimezone(tz=ept).strftime(
                "%Y-%m-%d %H:%M:%S"
            )
            try:
                if path.name.endswith(".pkl"):
                    out, errs = pickle.loads(content)
                    assert isinstance(out, list)
                    assert isinstance(out[0], list)
                elif path.name.endswith(".sql"):
                    out = load_sql(StringIO(content))
                elif path.name.endswith(".csv"):
                    out = pd.read_csv(StringIO(content))
                else:
                    raise ValueError(f"Unsupported file type: {path.name}")
            except (AssertionError, ValueError) as e:
                out = None
                logging.error(
                    f"failed to load {path} from {self.repo} with error: {e}"
                )
                comment = f"Found but failed to load {path.name}: please use `save_queries_and_records`"
            return out, comment

        self.result_files_loaded = {
            file_name: _load(path)
            for file_name, path in self.result_files.items()
        }
        return self


def pull_submissions(org: Organization, prefix: str) -> List[Submission]:
    """step 0. pull submissions (IO bound)"""
    # TODO: threadpool
    subs = [
        Submission(
            name=repo.name,
            members=sorted(
                [
                    c.login
                    for c in repo.get_collaborators()
                    if c.login not in STAFF
                ]
            ),
            repo=repo,
        )
        for repo in org.get_repos()
        if repo.name.startswith(prefix)
    ]
    subs = [
        sub for sub in subs if not any(staff in sub.name for staff in STAFF)
    ]
    # occasionally there are orphaned repos with active students
    orphaned = [sub for sub in subs if len(sub.members) == 0]
    for sub in orphaned:
        logging.warning(
            f"Orphaned repo {sub.name} with no student collaborator. Ignoring."
        )
    subs = [sub for sub in subs if len(sub.members) > 0]
    return subs


@dataclasses.dataclass
class LeaderboardEntry:
    leaderboard: str
    method: str
    dataset: str
    score: float
    member: str
    comment: str


def compute_entries(
    sub: Submission,
    test_data: Dict[str, pd.DataFrame],
    leaderboard_assignment_name: str,
) -> List[LeaderboardEntry]:
    """step 3. compute scores from dataframes (CPU bound)"""
    if not sub.result_files_loaded:
        return []

    def _f(file_name: str, model_records_and_comment) -> LeaderboardEntry:
        model_records, comment = model_records_and_comment
        method = file_name.removesuffix("_test.pkl")
        try:
            if model_records is None:
                score = -1.0
            else:
                score = compute_record_F1(test_data["records"], model_records)
        except ValueError as e:
            score = -1.0
            logging.error(
                f"failed to compute score for {file_name} from {sub.repo} with ValueError: {e}"
            )
        except KeyError as e:
            score = -1.0
            logging.error(
                f"failed to compute score for {file_name} from {sub.repo} with KeyError: {e}"
            )
        return LeaderboardEntry(
            leaderboard=f"{leaderboard_assignment_name}/{method}.csv",
            method=method,
            dataset=None,
            score=round(score, 4),
            member=", ".join(sub.members),
            comment=comment,
        )

    return [
        _f(k, v) for k, v in sub.result_files_loaded.items()
    ]


def aggregate_leaderboards(
    all_entries: List[LeaderboardEntry],
) -> Dict[str, pd.DataFrame]:
    """aims to be assignment-agonistic"""

    all_entries = [dataclasses.asdict(e) for e in all_entries]
    leaderboards = pd.DataFrame(all_entries)
    ret = {}
    for path, board in leaderboards.groupby("leaderboard"):
        board.sort_values("score", ascending=False, inplace=True)
        board = board[["score", "method", "member", "comment"]]
        board = board.rename(
            columns={
                "comment": "comment (last modified in ET or suggested errors)"
            }
        )
        board.reset_index(drop=True, inplace=True)
        ret[path] = board
    return ret


def update_leaderboard(
    path: str, board: pd.DataFrame, leaderboard_repo: Repository, deploy: bool
):
    csv_content = board.to_csv(index=False)

    full_path = os.path.join("public", path)
    os.makedirs(os.path.dirname(full_path), exist_ok=True)
    with open(full_path, "w+") as f:
        f.write(csv_content)
    logging.info(f"Updated {full_path}")

    if deploy:
        leaderboard_file = leaderboard_repo.get_contents(path)
        commit_message = "Leaderboard Update"
        leaderboard_repo.update_file(
            leaderboard_file.path,
            commit_message,
            csv_content,
            leaderboard_file.sha,
        )
        logging.info(f"Updated {path} in {leaderboard_repo}")


if __name__ == "__main__":
    logging.getLogger().setLevel(logging.INFO)

    parser = argparse.ArgumentParser()
    parser.add_argument("--gh_user", type=str)
    parser.add_argument("--gh_token", type=str)
    parser.add_argument("--deploy", action="store_true")
    args = parser.parse_args()

    GH_USER = os.environ.get("GH_USER") or args.gh_user
    GH_TOKEN = os.environ.get("GH_TOKEN") or args.gh_token
    DEPLOY = args.deploy
    REPO_ASSIGNMENT_PREFIX = "a5-"  # prefix of student repos
    LEADERBOARD_ASSIGNMENT_NAME = "a5"  # folder name in leaderboards repo
    logging.info(f"GH_USER: {GH_USER}, DEPLOY: {DEPLOY}")

    # assignment specific
    accepted_file_paths = [
        "records/t5_ft_test.pkl",
        "records/t5_scr_test.pkl",
        "records/llm_test.pkl",
    ]
    test_data = {
        # "sql_query": load_sql("data/a5/test.sql"),
        "records": load_records("data/a5/test_gt_records.pkl"),
    }

    gh = Github(GH_USER, GH_TOKEN)
    org = gh.get_organization(CLASS)
    subs = pull_submissions(org, REPO_ASSIGNMENT_PREFIX)
    logging.info(
        f"Found {len(subs)} submissions with prefix {REPO_ASSIGNMENT_PREFIX}"
    )
    if len(subs) == 0:
        logging.info("Exiting. No student submissions yet.")
        exit()

    logging.info("Pulling content ...")

    def pull_per_sub(sub: Submission):
        sub.pull_results(accepted_file_paths=accepted_file_paths)
        sub.pull_result_contents()
        logging.info(f"Pulled results for {sub.name}: {sub.result_files}.")

    with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:
        executor.map(pull_per_sub, subs)

    logging.info("Computing entries...")
    all_entries = []
    for sub in tqdm(subs, desc="computing entries"):
        all_entries += compute_entries(
            sub, test_data, LEADERBOARD_ASSIGNMENT_NAME
        )

    logging.info("Aggregating leaderboards...")
    all_leaderboards = aggregate_leaderboards(all_entries)

    leaderboard_repo = org.get_repo(LEADERBOARD_REPO_NAME) if DEPLOY else None
    for path, board in all_leaderboards.items():
        update_leaderboard(path, board, leaderboard_repo, DEPLOY)
    logging.info("Done.")
