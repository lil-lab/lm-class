################################################################################
# Imports.
################################################################################

import base64
from io import StringIO
import pandas as pd
from tqdm import tqdm
from github import Github
from sys import argv
import argparse
import pickle
import numpy as np
from typing import List, Any

################################################################################
# TODO: Configuration.
################################################################################


# argparsing for github token
parser = argparse.ArgumentParser()
parser.add_argument('--username', help='GitHub username')
parser.add_argument('--token', help='Access token for that GitHub username')
args = parser.parse_args()

# Write leaderboards to disk in current directory.

DRY_RUN = (argv[1] == "True")

# Username / password or access token
# See: https://github.com/PyGithub/PyGithub#simple-demo

GITHUB_TOKEN = [args.username, args.token]

# Organization name for the class.

CLASS = "cornell-cs5740-sp24"

# Exclude these people from collaborator list.

STAFF = {
    "yoavartzi",
    "momergul",
    "annshin",
    "Vrownie",
    "sy464",
    "YiChen8185",
    "kanlanc",
}

# Name of the leaderboard repo.

LEADERBOARD_REPO_NAME = "leaderboards"

# Assignment directory in leaderboard repo.

LEADERBOARD_ASSIGMENT_NAME = "a4"

# GitHub Classroom prefix attached to every repo, used to find assigments.

REPO_ASSIGNMENT_PREFIX = "cs5740-sp24-assignment-4-"

################################################################################
# TODO: Compute and sort scores.
################################################################################

def read_queries(sql_path: str):
    '''
    Same function than the one in 'utils.py' from the starter code.
    Unused for now, but could be useful if we need to compute the queries EM match.
    '''
    with open(sql_path, 'r') as f:
        qs = [q.strip() for q in f.readlines()]
    return qs

def load_records(record_path: str):
    try:
        with open(record_path, 'rb') as f:
            records, error_msgs = pickle.load(f)
    except:
        records = None
    return records

def compute_record_F1(gt_records: List[Any], model_records: List[Any]):
    '''
    Note: this is the exact same code from 'utils.py' file in the starter code.
    Copied here to avoid potential import issues.

    Helper function to compute F1 between records
    generated by ground-truth and model SQL queries
    '''
    F1s = []
    for gt_rec, model_rec in zip(gt_records, model_records):
        gt_set = set(gt_rec)
        model_set = set(model_rec)        

        precision_total = len(model_set)
        if precision_total == 0:
            precision = 1
        else:
            precision = len([rec for rec in model_set if rec in gt_set]) / precision_total
    
        recall_total = len(gt_set)    
        if recall_total == 0:
            recall = 1
        else:
            recall = len([rec for rec in gt_set if rec in model_set]) / recall_total

        F1 = 2 * precision * recall / (precision + recall + 1e-8)
        F1s.append(F1)

    return np.mean(F1s)


test_sql_path = "../a4/test_data/test.sql"
test_records_path = "../a4/test_data/test_gt_records.pkl"

test_records = load_records(test_records_path)
assert test_records is not None, "Error loading test records!"


def compute_scores(model_name, pred, repo):
    '''
    model_name: 
        Possible values: 'llm', 't5_ft', 't5_scr'
    pred: List[List]
        The matching records that the model predicted)
    repo: Dict
        All the info from the repo.
        Keys in the repo: dict_keys(['git', 'name', 'team', 'files', 'results'])
    '''
    comment = ""

    if pred is None:
        score = None
        comment = "Error reading records!"
    elif repo['dummy'][model_name]:
        score = 0.00000
        # comment = "Dummy file used!"
    else:
        try:
            score = compute_record_F1(gt_records=test_records, model_records=pred)
            score = score.round(5)
        except:
            score = None
            comment = "Error computing F1!"
    
    return {
        # Required: name of leaderboard file.
        "leaderboard": "leaderboard_a4",
        "Record F1":     f'{score:.5f}',
        "Method":      model_name,
        "Member":      " ".join(repo["team"]),
        "Comment":      comment,
    }

def sort_scores(leaderboards):

    if len(leaderboards) == 0:
        return leaderboards

    return (
        leaderboards
        .sort_values([
            "Record F1",
            "Member",
        ], ascending = False)
    )

################################################################################
# API authentication, find organization and leaderboard repo.
################################################################################

git = Github(*GITHUB_TOKEN)
org = git.get_organization(CLASS)
leaderboard_repo = org.get_repo(LEADERBOARD_REPO_NAME)

################################################################################
# Get all assignment repos, team names, members, etc.
################################################################################

print("Loading Repos...")

repos = [

    {

        "git":  repo,
        "name": repo.name,
        "team": sorted([
            c.login for c in repo.get_collaborators()
            if c.login not in STAFF
        ]),

    }

    for repo in org.get_repos()
    if repo.name.startswith(REPO_ASSIGNMENT_PREFIX)

]

# Remove all staff member teams.

repos = [repo for repo in repos if len(repo["team"]) > 0]

################################################################################
# Extract repo files.
################################################################################

possible_files = [
    "t5_ft_test.pkl",
    "t5_scr_test.pkl",
    "llm_test.pkl",
]

for repo in tqdm(repos, desc = "Finding files"):

    repo["files"] = {
        
        result_file.name: result_file
        for result_file in repo["git"].get_contents("records")
        if result_file.name.endswith(".pkl") and result_file.name in possible_files

    }

################################################################################
# Download files and load CSVs.
################################################################################

for repo in tqdm(repos, desc = "Downloading files"):
    print(repo['name'])

    repo["results"] = {
        "llm": {},
        "t5_ft": {},
        "t5_scr": {},
    }
    repo["dummy"] = {
        "llm": None,
        "t5_ft": None,
        "t5_scr": None,
    }

    for file_name, path in repo["files"].items():

        content_encoded = repo["git"].get_git_blob(path.sha).content
        content = base64.b64decode(content_encoded)

        model_name = None
        try:
            model_name = file_name.split("_test.pkl")[0]
        except:
            print(f"Error getting model name from file name {file_name}!")
        
        data = None
        if model_name:
            try:
                data, error_messages = pickle.loads(content)

                # Used to check if they used the dummy file
                if len(set(error_messages)) == 1 and \
                    error_messages[0] == "Dummy error message":
                    repo["dummy"][model_name] = True
            except:
                print(f"Error loading the records pickle {file_name}!")
            
            repo["results"][model_name] = data


################################################################################
# Compute scores and create assignment-level master leaderboard.
################################################################################

print("======================= Computing Scores =======================")

leaderboards = []
for repo in repos:
    print(repo['name'])
    for model_name, result in repo["results"].items():
        print(model_name)
        try:
            score_dict = compute_scores(model_name, result, repo)
        except:
            print(f"Error computing score for {model_name} in {repo['name']}!")
            continue

        if score_dict is not None:
            leaderboards.append(score_dict)

leaderboards = sort_scores(pd.DataFrame(leaderboards))

################################################################################
# Split master leaderboard into sub-boards and commit them.
################################################################################

if len(leaderboards) != 0:

    for name, board in leaderboards.groupby("leaderboard"):

        del board["leaderboard"]

        csv_content = board.to_csv(index = False)
        csv_name    = name + ".csv"

        commit_message = "Leaderboard Update"

        if DRY_RUN:

            with open("public/" + csv_name, "w") as f:
                f.write(csv_content)

        else:

            leaderboard_file = leaderboard_repo.get_contents(
                LEADERBOARD_ASSIGMENT_NAME + "/" + csv_name)

            print("Updating", leaderboard_file.path)

            leaderboard_repo.update_file(
                leaderboard_file.path,
                commit_message,
                csv_content,
                leaderboard_file.sha)

print("Done!")

################################################################################
################################################################################
################################################################################
################################################################################
################################################################################
