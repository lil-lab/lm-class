---
# Feel free to add content and custom Front Matter to this file.
# To modify the layout, see https://jekyllrb.com/docs/themes/#overriding-theme-defaults

layout: page
title: Lecture Slides
---

## Warming Up

This section quickly brings the students up to speed with the basics. The goal is to prepare the students for the first assignment. Beyond a quick introduction, it includes: data basics, linear perceptron, and multi-layer perceptron.

1. Introduction [[key](lectures/01 - intro.key)] [[pdf](lectures/01 - intro.pdf)]
1. Text Classification, Data Basics, and Perceptrons [[key](lectures/02%20-%20data%20basics%20and%20perceptron.key)] [[pdf](lectures/02%20-%20data%20basics%20and%20perceptron.pdf)]
1. Neural Network Basics [[key](lectures/03%20-%20neural%20networks.key)] [[pdf](lectures/03%20-%20neural%20networks.pdf)]

## Learning with Raw Data

This section focuses on representation learning from raw data (i.e., without any annotation or user labor). It is divided into three major parts: word embeddings, next-word-prediction language modeling, and masked language modeling. Through these subsections we introduce many of the fundamental technical concepts and methods of the field.

1. Word Embeddings [[key](lectures/04%20-%20word%20embeddings.key)] [[pdf](lectures/04%20-%20word%20embeddings.pdf)]
1. N-gram Language Models [[key](lectures/05%20-%20language%20models.key)] [[pdf](lectures/05%20-%20language%20models.pdf)]
1. Tokenization [[key](lectures/06%20-%20tokenization.key)] [[pdf](lectures/06%20-%20tokenization.pdf)]
1. Neural Language Models and Transformers [[key](lectures/07%20-%20neural%20lms%20and%20transformers.key)] [[pdf](lectures/07%20-%20neural%20lms%20and%20transformers.pdf)]
1. Decoding LMs [[key](lectures/08%20-%20decoding%20lms.key)] [[pdf](lectures/08%20-%20decoding%20lms.pdf)]
1. Scaling up to LLMs [[key](lectures/09%20-%20scaling%20up%20to%20llms.key)] [[pdf](lectures/09%20-%20scaling%20up%20to%20llms.pdf)]
1. Masked Language Models and BERT [[key](lectures/10%20-%20masked%20lms.key)] [[pdf](lectures/10%20-%20masked%20lms.pdf)]
1. Pretraining Encoder-decoders [[key](lectures/11%20-%20encdec%20pretrain.key)] [[pdf](lectures/11%20-%20encdec%20pretrain.pdf)]
1. Working with Raw Data Recap [[key](lectures/12%20-%20raw%20data%20recap.key)] [[pdf](lectures/12%20-%20raw%20data%20recap.pdf)]

## Learning with Annotated Data

This section focuses on learning with annotated data. It introduces the task as a framework to structure solution development, through the review of several prototypical NLP tasks. For each task, we discuss the problem, data, modeling decisions, and formulate a technical approach to address it. This section takes a broad view of annotated data, including covering language model alignment using annotated data (i.e., instruction tuning and RLHF).

1. Prototypical NLP Tasks [[key](lectures/13%20-%20tasks.key)] [[pdf](lectures/13%20-%20tasks.pdf)]
1. Aligning LLMs [[key](lectures/14%20-%20aligning%20llms.key)] [[pdf](lectures/14%20-%20aligning%20llms.pdf)]
1. Working with LLMs: Prompting [[key](lectures/15%20-%20prompting.key)] [[pdf](lectures/15%20-%20prompting.pdf)]
